{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 5: Generalisation\n",
    "\n",
    "### Neil D. Lawrence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "- Last time: introduced basis functions.\n",
    "- Showed how to maximize the likelihood of a non-linear model that's linear in parameters.\n",
    "- Explored the different characteristics of different basis function models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial Fits to Olympics Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'GPy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bc9f1a0dc952>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmlai\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mteaching_plots\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\COMP6509_week4\\week5_2018\\teaching_plots.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmplot3d\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAxes3D\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mGPy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#import daft\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'GPy'"
     ]
    }
   ],
   "source": [
    "import pods\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import mlai\n",
    "from matplotlib import pyplot as plt\n",
    "import teaching_plots as plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "max_basis = 5\n",
    "basis = mlai.polynomial\n",
    "\n",
    "data = pods.datasets.olympic_marathon_men()\n",
    "x = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "data_limits = [1892, 2020]\n",
    "num_data = x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b1120165430d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m plot.rmse_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1), \n\u001b[0m\u001b[0;32m      2\u001b[0m               \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmlai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbasis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_limits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_limits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m               xlim=data_limits, objective_ylim=[0, 10])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot' is not defined"
     ]
    }
   ],
   "source": [
    "plot.rmse_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1), \n",
    "              model=mlai.LM, basis=basis, data_limits=data_limits, \n",
    "              xlim=data_limits, objective_ylim=[0, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "def display_plots(filebase, directory=None, width=700, height=500, **kwargs):\n",
    "    \"\"\"Display a series of plots controlled by sliders. The function relies on Python string format functionality to index through a series of plots.\"\"\"\n",
    "    def show_figure(filebase, directory, **kwargs):\n",
    "        \"\"\"Helper function to load in the relevant plot for display.\"\"\"\n",
    "        filename = filebase.format(**kwargs)\n",
    "        if directory is not None:\n",
    "            filename = directory + '/' + filename           \n",
    "        display(HTML(\"<img src='{filename}'>\".format(filename=filename)))\n",
    "        \n",
    "    interact(show_figure, filebase=fixed(filebase), directory=fixed(directory), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "display_plots('olympic_LM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', num_basis=(1, max_basis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting\n",
    "- Increase number of basis functions we obtain a better 'fit' to the data.\n",
    "- How will the model perform on previously unseen data?\n",
    "- Let's consider predicting the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.holdout_fit(x, y, param_name='num_basis', \n",
    "                 param_range=(1, max_basis+1), \n",
    "                 model=mlai.LM, basis=basis, data_limits=data_limits,\n",
    "                 permute=False, objective_ylim=[0, 20], xlim=data_limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extrapolation\n",
    "\n",
    "- Here we are training beyond where the model has learnt.\n",
    "- This is known as *extrapolation*.\n",
    "- Extrapolation is predicting into the future here, but could be:\n",
    "    - Predicting back to the unseen past (pre 1892)\n",
    "    - Spatial prediction (e.g. Cholera rates outside Manchester given rates inside Manchester)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Alan Turing\n",
    "- He was a formidable Marathon runner. \n",
    "- In 1946 he ran a time 2 hours 46 minutes.\n",
    "- What is the probability he would have won an Olympics if one had been held in 1946?  \n",
    "![Alan Turing running in 1946](http://www.turing.org.uk/turing/pi2/run.jpg)\n",
    "<center>*Alan Turing, in 1946 he was only 11 minutes slower than the winner of the 1948 games. Would he have won a hypothetical games held in 1946? Source: [Alan Turing Internet Scrapbook](http://www.turing.org.uk/scrapbook/run.html).*</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpolation\n",
    "- Predicting the wining time for 1946 Olympics is *interpolation*.\n",
    "- This is because we have times from 1936 and 1948.\n",
    "- If we want a model for *interpolation* how can we test it?\n",
    "- One trick is to sample the validation set from throughout the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.holdout_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1), \n",
    "                 model=mlai.LM, basis=basis, data_limits=data_limits, \n",
    "                 xlim=data_limits, prefix='olympic_val_inter', objective_ylim=[0.0, 2.], permute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "display_plots('olympic_val_inter_LM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', num_basis=(1, max_basis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choice of Validation Set\n",
    "\n",
    "- The choice of validation set should reflect how you will use the model in practice.\n",
    "- For extrapolation into the future we tried validating with data from the future.\n",
    "- For interpolation we chose validation set from data.\n",
    "- For different validation sets we could get different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leave One Out Error\n",
    "- Take training set and remove one point.\n",
    "- Train on the remaining data.\n",
    "- Compute the error on the point you removed (which wasn't in the training data).\n",
    "- Do this for each point in the training set in turn.\n",
    "- Average the resulting error. \n",
    "- This is the leave one out error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.loo_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1),  \n",
    "             model=mlai.LM, basis=basis, data_limits=data_limits, \n",
    "             xlim=data_limits, objective_ylim=[0., 2.], prefix='olympic_loo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "display_plots('olympic_loo{part:0>3}_LM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', num_basis=(1, max_basis), part=(0,x.shape[0]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias Variance Decomposition\n",
    "\n",
    "Expected test error for different variations of the *training data* sampled from, $\\Pr(\\mathbf{x}, y)$\n",
    "\n",
    "$$\\mathbb{E}\\left[ (y - f^*(\\mathbf{x}))^2 \\right]$$\n",
    "\n",
    "Decompose as\n",
    "\n",
    "$$\\mathbb{E}\\left[ (y - f(\\mathbf{x}))^2 \\right] = \\text{bias}\\left[f^*(\\mathbf{x})\\right]^2 + \\text{variance}\\left[f^*(\\mathbf{x})\\right] +\\sigma^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias\n",
    "\n",
    "- Given by\n",
    "    $$\\text{bias}\\left[f^*(\\mathbf{x})\\right] = \\mathbb{E}\\left[f^*(\\mathbf{x})\\right] - f(\\mathbf{x})$$\n",
    "    \n",
    "- Error due to bias comes from a model that's too simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variance\n",
    "\n",
    "- Given by\n",
    "    $$\\text{variance}\\left[f^*(\\mathbf{x})\\right] = \\mathbb{E}\\left[\\left(f^*(\\mathbf{x}) -  \\mathbb{E}\\left[f^*(\\mathbf{x})\\right]\\right)^2\\right]$$\n",
    "    \n",
    "- Slight variations in the training set cause changes in the prediction. Error due to variance is error in the model due to an overly complex model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$ Fold Cross Validation\n",
    "\n",
    "- Leave one out error can be very time consuming.\n",
    "- Need to train your algorithm $n$ times.\n",
    "- An alternative: $k$ fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.cv_fit(x, y, param_name='num_basis', param_range=(1, max_basis+1),  \n",
    "               model=mlai.LM, basis=basis, data_limits=data_limits,\n",
    "               xlim=data_limits, objective_ylim=[0.,2.], num_parts=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "display_plots('olympic_{num_parts}'.format(num_parts=5) + 'cv{part:0>2}_LM_polynomial_num_basis{num_basis:0>3}.svg', \n",
    "                            directory='./diagrams', part=(0,4),num_basis=(1, max_basis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "- Section 1.5 of Rogers and Girolami (2016)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
